var documenterSearchIndex = {"docs":
[{"location":"#MirrorVI.jl","page":"MirrorVI.jl","title":"MirrorVI.jl","text":"","category":"section"},{"location":"","page":"MirrorVI.jl","title":"MirrorVI.jl","text":"Documentation for MirrorVI.jl","category":"page"},{"location":"","page":"MirrorVI.jl","title":"MirrorVI.jl","text":"Modules = [MirrorVI, MyOptimisers]","category":"page"},{"location":"#MirrorVI.ConstantDistribution","page":"MirrorVI.jl","title":"MirrorVI.ConstantDistribution","text":"Univariate Constant Distribution\n\n\n\n\n\n","category":"type"},{"location":"#MirrorVI.compute_logpdf_prior-Tuple{ComponentArrays.ComponentArray}","page":"MirrorVI.jl","title":"MirrorVI.compute_logpdf_prior","text":"compute_logpdf_prior(theta::ComponentArray; params_dict::OrderedDict)\n\nCompute the sum of the log-pdf of the prior distribution.\n\nArguments\n\ntheta::ComponentArray: ComponentArray with the components of the parameters sample, one component for each prior.\nparams_dict::OrderedDict: OrderedDict generated using the MirrorVI.utils functions containing the prior details.\n\nReturns\n\nReal: The sum of the log-pdf components.\n\n\n\n\n\n","category":"method"},{"location":"#MirrorVI.cyclical_polynomial_decay","page":"MirrorVI.jl","title":"MirrorVI.cyclical_polynomial_decay","text":"cyclical_polynomial_decay(n_iter::Int64, n_cycles::Int64=2)\n\nGenerate a cyclical polynomial decay schedule over n_iter iterations, divided into n_cycles cycles.\n\nThis function creates a learning rate schedule where the polynomial decay is applied cyclically.     Each cycle consists of steps_per_cycle = n_iter / n_cycles steps, and the polynomial_decay function is applied within each cycle.\n\nArguments\n\nn_iter::Int64: The total number of iterations for the schedule.\nn_cycles::Int64=2: The number of cycles to divide the iterations into. Default is 2.\n\nReturns\n\nVector{Float32}: A vector of decayed values representing the learning rate schedule.\n\nExamples\n\n```julia julia> schedule = cyclicalpolynomialdecay(10, 2)  # 10 iterations, 2 cycles 10-element Vector{Float32}:  0.17782794  0.125  0.09765625  0.080566406  0.06871948  0.17782794  0.125  0.09765625  0.080566406  0.06871948\n\njulia> length(schedule)  # Total number of iterations 10\n\n\n\n\n\n","category":"function"},{"location":"#MirrorVI.elbo-Tuple{AbstractArray}","page":"MirrorVI.jl","title":"MirrorVI.elbo","text":"elbo(\n    z::AbstractArray;\n    y::AbstractArray,\n    X::AbstractArray,\n    ranges_z::AbstractArray,\n    vi_family_array::AbstractArray,\n    random_weights::AbstractArray,\n    model,\n    theta_axes::Tuple,\n    log_likelihood,\n    log_prior=zero,\n    n_samples::Int64=1,\n    n_repeated_measures::Int64=1\n)\n\nCompute the Evidence Lower Bound (ELBO) for a variational inference problem.\n\nThe ELBO is a key quantity in variational inference, used to approximate the posterior distribution of model parameters. This function computes the ELBO by:\n\nSampling from the variational distribution.\nEvaluating the log-likelihood and log-prior of the model.  Optional: Takes into account repeated measurements, if it is required\nAdding the entropy of the variational distribution.\n\nArguments\n\nz::AbstractArray: The variational parameters used to define the variational distribution.\ny::AbstractArray: The observed data (target values).\nX::AbstractArray: The input data (features).\nranges_z::AbstractArray: Specifies how z is divided among the parameters of the variational distribution.\nvi_family_array::AbstractArray: An array of functions defining the variational family for each parameter.\nrandom_weights::AbstractArray: Boolean array of the same dimension as theta, stating whether each parameter is random or not.\nmodel: A function representing the model, which takes parameters and input data X (keyword argument) and returns atuple with the predictions.\ntheta_axes::ComponentArrays.Axes: The axes for constructing a ComponentArray from the sampled parameters.\nlog_likelihood: A function that computes the log-likelihood of the observed data given the model predictions.\nlog_prior=zero: A function that computes the log-prior of the parameters. Default is zero (no prior).\nn_samples::Int64=1: The number of Monte Carlo samples to use for approximating the ELBO. Default is 1.\nn_repeated_measures::Int64=1: The number of repeated measurements (e.g., for longitudinal data). Default is 1.\n\nReturns\n\nFloat64: The negative ELBO value (to be minimized).\n\n\n\n\n\n","category":"method"},{"location":"#MirrorVI.get_parameters_axes-Tuple{OrderedCollections.OrderedDict}","page":"MirrorVI.jl","title":"MirrorVI.get_parameters_axes","text":"get_parameters_axes(params_dict::OrderedDict)\n\nGenerate the parameters axes as needed by the library ComponentArrays. This function processes a dictionary of parameters (params_dict) and constructs a prototype array with the same structure as the parameters. The axes of this prototype array are returned, which can be used to initialize a ComponentArray with the correct structure.\n\nArguments\n\nparams_dict::OrderedDict: OrderedDict generated using the MirrorVI.utils functions containing the prior details.\n\nReturns\n\nTuple: The axes of the prototype array, which can be used to initialize a ComponentArray with the same structure as the parameters.\n\n\n\n\n\n","category":"method"},{"location":"#MirrorVI.hybrid_training_loop-Tuple{}","page":"MirrorVI.jl","title":"MirrorVI.hybrid_training_loop","text":"hybrid_training_loop(;\n    z::AbstractArray,\n    y::AbstractArray,\n    X::AbstractArray,\n    params_dict::OrderedDict,\n    model,\n    log_likelihood,\n    log_prior=zero,\n    n_iter::Int64,\n    optimiser::Optimisers.AbstractRule,\n    save_all::Bool=false,\n    use_noisy_grads::Bool=false,\n    elbo_samples::Int64=1,\n    lr_schedule=nothing,\n    n_repeated_measures::Int64=1,\n    dropout::Bool=false,\n    start_dropout_iter::Int=0\n)\n\nRun a training loop for variational inference, combining gradient-based optimization with optional noise injection and dropout (experimental).\n\nThis function performs variational inference by minimizing the Evidence Lower Bound (ELBO) using gradient-based optimization. It supports:\n\nNoisy gradients for exploration.\nDropout for regularization.\nCyclical learning rate schedules.\nSaving intermediate results for analysis.\n\nArguments\n\nz::AbstractArray: Initial variational parameters.\ny::AbstractArray: Observed data (target values).\nX::AbstractArray: Input data (features).\nparams_dict::OrderedDict: Ordered Dictionary defined through MirrorVI.utils functions, containing configuration for the variational family, parameter ranges, and other settings. Must include:\n\"vi_family_array\": Array of functions defining the variational family for each parameter.\n\"ranges_z\": Specifies how z is divided among the parameters of the variational distribution.\n\"random_weights\": Weights used for sampling from the variational distribution.\n\"noisy_gradients\": Standard deviation of noise added to gradients (if use_noisy_grads=true).\nmodel: A function representing the model, which takes parameters and input data X and returns predictions.\nlog_likelihood: A function that computes the log-likelihood of the observed data given the model predictions.\nlog_prior=zero: A function that computes the log-prior of the parameters. Default is zero (no prior).\nn_iter::Int64: Number of training iterations.\noptimiser::Optimisers.AbstractRule: Optimiser to use for updating z (e.g., DecayedADAGrad).\nsave_all::Bool=false: If true, saves the trace of z across all iterations. Default is false.\nuse_noisy_grads::Bool=false: If true, adds noise to the gradients during training. Default is false.\nelbo_samples::Int64=1: Number of Monte Carlo samples to use for approximating the ELBO. Default is 1.\nlr_schedule=nothing: Learning rate schedule (e.g., from cyclical_polynomial_decay). Default is nothing.\nn_repeated_measures::Int64=1: Number of repeated measurements (e.g., for time-series data). Default is 1.\ndropout::Bool=false: If true, applies dropout to z during training. Default is false.\nstart_dropout_iter::Int=0: Iteration at which to start applying dropout. Default is 0.\n\nReturns\n\nDict: A dictionary containing:\n\"loss_dict\": A dictionary with keys:\n\"loss\": Array of ELBO values across iterations.\n\"z_trace\": Trace of z across iterations (if save_all=true).\n\"best_iter_dict\": A dictionary with keys:\n\"best_loss\": Best ELBO value achieved.\n\"best_z\": Variational parameters corresponding to the best ELBO.\n\"final_z\": Final variational parameters after training.\n\"best_iter\": Iteration at which the best ELBO was achieved.\n\n\n\n\n\n","category":"method"},{"location":"#MirrorVI.polynomial_decay-Tuple{Int64}","page":"MirrorVI.jl","title":"MirrorVI.polynomial_decay","text":"polynomial_decay(t::Int64; a::Float32=1f0, b::Float32=0.01f0, gamma::Float32=0.75f0)\n\nCompute the polynomial decay value at step t using the formula: a * (b + t)^(-gamma)\n\nThis function is commonly used in optimization algorithms (e.g., learning rate scheduling) to decay a value polynomially over time.\n\n# Arguments\n- `t::Int64`: The current step or time at which to compute the decay.\n- `a::Float32=1f0`: The initial scaling factor. Default is `1.0`.\n- `b::Float32=0.01f0`: A small constant to avoid division by zero. Default is `0.01`.\n- `gamma::Float32=0.75f0`: The decay rate. Controls how quickly the value decays. Default is `0.75`.\n\n# Returns\n- `Float32`: The decayed value at step `t`.\n\n# Examples\n```julia\njulia> polynomial_decay(10)  # Default parameters\n0.17782794f0\n\njulia> polynomial_decay(100, a=2.0f0, b=0.1f0, gamma=0.5f0)  # Custom parameters\n0.31622776f0\n\n\n\n\n\n","category":"method"},{"location":"#MirrorVI.MyOptimisers.DecayedADAGrad","page":"MirrorVI.jl","title":"MirrorVI.MyOptimisers.DecayedADAGrad","text":"DecayedADAGrad(; η = 0.1, pre = 1.0, post = 0.9)\n\nImplements a decayed version of AdaGrad. It has parameter specific learning rates based on how frequently it is updated. Does not really need tuning. References: ADAGrad optimiser.\n\nArguments\n\nη=0.1: learning rate\npre=1.0: weight of new gradient norm\npost=0.9: weight of histroy of gradient norms\n\nReturns\n\nVector{Float32}:\n\n\n\n\n\n","category":"type"}]
}
